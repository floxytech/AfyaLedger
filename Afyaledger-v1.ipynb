"{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"fa4f64ef\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Afyaledger - Model Training (Afyaledger-v1)\\n\",\n    \"\\n\",\n    \"This notebook trains the fusion model (text embeddings + numeric features) using the Afyaledger CSV dataset. Designed to run in Watson Studio or locally.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"288e59d0\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Install required packages (uncomment if needed)\\n\",\n    \"# !pip install torch transformers sentence-transformers sklearn pandas joblib\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"84d5858b\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import os\\n\",\n    \"import pandas as pd\\n\",\n    \"import numpy as np\\n\",\n    \"import torch\\n\",\n    \"import torch.nn as nn\\n\",\n    \"from torch.utils.data import Dataset, DataLoader\\n\",\n    \"from sentence_transformers import SentenceTransformer\\n\",\n    \"from sklearn.preprocessing import StandardScaler\\n\",\n    \"from sklearn.model_selection import train_test_split\\n\",\n    \"import joblib\\n\",\n    \"\\n\",\n    \"DATA_DIR = '/mnt/data'  # adjust when running in Watson Studio to the project storage path\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"f0248718\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# --- Load data ---\\n\",\n    \"companies = pd.read_csv(os.path.join(DATA_DIR, 'companies.csv'))\\n\",\n    \"income = pd.read_csv(os.path.join(DATA_DIR, 'income_statements.csv'))\\n\",\n    \"balance = pd.read_csv(os.path.join(DATA_DIR, 'balance_sheets.csv'))\\n\",\n    \"cashflow = pd.read_csv(os.path.join(DATA_DIR, 'cashflow_statements.csv'))\\n\",\n    \"notes = pd.read_csv(os.path.join(DATA_DIR, 'notes_texts.csv'))\\n\",\n    \"labels = pd.read_csv(os.path.join(DATA_DIR, 'labels.csv'))\\n\",\n    \"features = pd.read_csv(os.path.join(DATA_DIR, 'engineered_features.csv'))\\n\",\n    \"\\n\",\n    \"print('Loaded datasets:', [len(df) for df in [companies,income,balance,cashflow,notes,labels,features]])\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"98010067\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# --- Simple merge for training ---\\n\",\n    \"df = features.merge(notes[['company_id','period_end','text']], on=['company_id','period_end'], how='left')\\n\",\n    \"df = df.merge(labels[['company_id','period_end','target_default_12m','next_12m_return']], on=['company_id','period_end'], how='left')\\n\",\n    \"df['text'] = df['text'].fillna('')\\n\",\n    \"\\n\",\n    \"df.head()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"976ffa11\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# --- Prepare numeric X and labels ---\\n\",\n    \"NUM_COLS = ['gross_margin','net_margin','roe','roa','current_ratio','debt_to_equity','fcf_margin','delta_revenue','rolling_revenue_mean_4','rolling_margin_trend_slope']\\n\",\n    \"X_num = df[NUM_COLS].fillna(0.0).astype(float)\\n\",\n    \"scaler = StandardScaler()\\n\",\n    \"X_num_scaled = scaler.fit_transform(X_num)\\n\",\n    \"joblib.dump(scaler, 'scaler.joblib')\\n\",\n    \"\\n\",\n    \"y = df['target_default_12m'].fillna(0).astype(int).values\\n\",\n    \"texts = df['text'].tolist()\\n\",\n    \"\\n\",\n    \"print('Numeric shape:', X_num_scaled.shape)\\n\",\n    \"print('Texts:', len(texts))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"66d0b351\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# --- Dataset + Model definitions ---\\n\",\n    \"class FinDataset(Dataset):\\n\",\n    \"    def __init__(self, X_num, texts, y, embedder):\\n\",\n    \"        self.X_num = X_num.astype('float32')\\n\",\n    \"        self.texts = texts\\n\",\n    \"        self.y = y.astype('float32')\\n\",\n    \"        self.embedder = embedder\\n\",\n    \"    def __len__(self):\\n\",\n    \"        return len(self.y)\\n\",\n    \"    def __getitem__(self, idx):\\n\",\n    \"        num = self.X_num[idx]\\n\",\n    \"        emb = self.embedder.encode(self.texts[idx], convert_to_numpy=True)\\n\",\n    \"        return torch.from_numpy(num), torch.from_numpy(emb).float(), torch.tensor(self.y[idx])\\n\",\n    \"\\n\",\n    \"class FusionModel(nn.Module):\\n\",\n    \"    def __init__(self, text_dim=384, num_dim=10, hidden=256):\\n\",\n    \"        super().__init__()\\n\",\n    \"        self.num_net = nn.Sequential(nn.Linear(num_dim,128), nn.ReLU(), nn.Linear(128,64))\\n\",\n    \"        self.fusion = nn.Sequential(nn.Linear(text_dim+64,hidden), nn.ReLU(), nn.Dropout(0.2), nn.Linear(hidden,hidden//2), nn.ReLU())\\n\",\n    \"        self.classifier = nn.Linear(hidden//2,1)\\n\",\n    \"    def forward(self, x_num, x_text):\\n\",\n    \"        n = self.num_net(x_num)\\n\",\n    \"        x = torch.cat([x_text, n], dim=1)\\n\",\n    \"        x = self.fusion(x)\\n\",\n    \"        out = torch.sigmoid(self.classifier(x)).squeeze(1)\\n\",\n    \"        return out\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"73919bc8\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# --- Prepare embedding model and dataloaders ---\\n\",\n    \"embedder = SentenceTransformer('all-MiniLM-L6-v2')\\n\",\n    \"\\n\",\n    \"X_train_num, X_val_num, t_train, t_val, y_train, y_val = train_test_split(\\n\",\n    \"    X_num_scaled, texts, y, test_size=0.2, random_state=42, stratify=y if len(set(y))>1 else None\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"train_ds = FinDataset(X_train_num, t_train, y_train, embedder)\\n\",\n    \"val_ds = FinDataset(X_val_num, t_val, y_val, embedder)\\n\",\n    \"\\n\",\n    \"train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\\n\",\n    \"val_loader = DataLoader(val_ds, batch_size=16, shuffle=False)\\n\",\n    \"\\n\",\n    \"print('Train size:', len(train_ds), 'Val size:', len(val_ds))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"ad38e60a\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# --- Instantiate model, optimizer, loss ---\\n\",\n    \"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\",\n    \"model = FusionModel(text_dim=embedder.get_sentence_embedding_dimension(), num_dim=X_train_num.shape[1], hidden=256).to(device)\\n\",\n    \"opt = torch.optim.AdamW(model.parameters(), lr=1e-4)\\n\",\n    \"loss_fn = nn.BCELoss()\\n\",\n    \"\\n\",\n    \"# quick training loop (few epochs)\\n\",\n    \"for epoch in range(5):\\n\",\n    \"    model.train()\\n\",\n    \"    total_loss = 0.0\\n\",\n    \"    for x_num, x_text, yb in train_loader:\\n\",\n    \"        x_num = x_num.to(device).float()\\n\",\n    \"        x_text = x_text.to(device).float()\\n\",\n    \"        yb = yb.to(device).float()\\n\",\n    \"        pred = model(x_num, x_text)\\n\",\n    \"        loss = loss_fn(pred, yb)\\n\",\n    \"        opt.zero_grad(); loss.backward(); opt.step()\\n\",\n    \"        total_loss += loss.item() * x_num.size(0)\\n\",\n    \"    print(f'Epoch {epoch+1} loss:', total_loss / len(train_ds))\\n\",\n    \"\\n\",\n    \"# Save model\\n\",\n    \"torch.save(model.state_dict(), 'afyaledger_fusion_model.pt')\\n\",\n    \"print('Model saved: afyaledger_fusion_model.pt')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"30b6532b\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Next steps\\n\",\n    \"- Evaluate on validation set and compute ROC-AUC, precision@K.\\n\",\n    \"- Add logging, early stopping, and checkpointing for production.\\n\",\n    \"- Replace `SentenceTransformer` with watsonx embeddings in Watson Studio if preferred.\"\n   ]\n  }\n ],\n \"metadata\": {},\n \"nbformat\": 4,\n \"nbformat_minor\": 5\n}"